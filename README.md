# TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network
---
This repository contains code from the paper "TTS-GAN: A Transformer-based Time-Series Generative Adversarial Network"

---

**Abstract:**
Time-series datasets used in machine learning applications often are small in size, making the training of deep neural network architectures ineffective. For time series, the suite of data augmentation tricks we can use to expand the size of the dataset is limited by the need to maintain the basic properties of the signal. Data generated by a Generative Adversarial Network (GAN) can be utilized as another data augmentation tool. RNN-based GANs suffer from the fact that they cannot effectively model long sequences of data points with irregular temporal relations. To tackle these problems, we introduce TTS-GAN, a transformer-based GAN which can successfully generate realistic synthetic time series data sequences of arbitrary length, similar to the original ones. Both the generator and discriminator networks of the GAN model are built using a pure transformer encoder architecture. We use visualizations to demonstrate the similarity of real and generated time series and a simple classification task that shows how we can use synthetically generated data to augment real data and improve classification accuracy.

---

**Key Idea:**

Transformer GAN generate synthetic time-series data

The TTS-GAN Architecture 

![The TTS-GAN Architecture](./images/TTS-GAN.png)

The time series data processing step

![The time series data processing step](./images/PositionalEncoding.png)

---

**Code Instructions:**


To train the Running data GAN model:
```
python RunningGAN_Train.py
```

To train the Jumping data GAN model:
```
python JumpingGAN_Train.py
```

A simple example of using pre-trained model to generate synthetic Running and Jumping data:
```
Running&JumpingVisualization.ipynb
```
